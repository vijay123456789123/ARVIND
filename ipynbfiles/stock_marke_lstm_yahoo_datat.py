# -*- coding: utf-8 -*-
"""Stock_Marke_LSTM_Yahoo_datat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M_XhwAwTnFwvu1NVhT0IWxm430ybNIoo

<a href="https://colab.research.google.com/github/arpit0891/Stock-price-predection-using-LSTM-and-Sentiment-analysis/blob/main/Stock_Marke_LSTM_Yahoo_datat.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import numpy as np
import pandas as pd
import os

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

from google.colab import drive
drive.mount('/content/drive')

data = '/content/drive/My Drive/LSTM_Input'

"""The dataset is tanken from https://in.finance.yahoo.com/

Any stock dataset you want to add in the website can be taken from the website given above then traing the model on that dataset
"""

for dirname, _, filenames in os.walk(data):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import pandas as pd
import numpy as np
import datetime as dt
from datetime import datetime

import matplotlib.pyplot as plt

import numpy as np

from sklearn.preprocessing import MinMaxScaler

### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from keras.layers import Dropout

print(data)

#drive.mount('/content/drive')
data = '/content/drive/My Drive/LSTM_Input/RELIANCE.csv'

df = pd.read_csv(data)

#print the head
df.head()

df['Date'] = pd.to_datetime(df.Date,format='%Y-%m-%d')
df.index = df['Date']
plt.figure(figsize=(20,8))
plt.plot(df['Close'], label='Close Price')

"""The sudden drop as seen on the graph was due to dilution in the shares. Later we tackled this problem by taking standard price for the complete graph"""

features = ["Date", "Close"]
all_data = df[features]
all_data.index = all_data.Date
all_data.drop('Date', axis=1, inplace=True)

all_data.head()
all_data.shape

"""Preprocessing the data"""

dataset = all_data.values
train = dataset[2000:4500,:]
valid = dataset[4500:,:]

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)

x_train, y_train = [], []
for i in range(90,len(train)):
    x_train.append(scaled_data[i-90:i,0])
    y_train.append(scaled_data[i,0])
x_train, y_train = np.array(x_train), np.array(y_train)
#we take the 90 days dataset and predict the 91st day
#Code for making csv for it will be reflected during the presentation part where
#We created a front end and backend to use the model we made here

x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))

inputs = all_data[len(all_data) - len(valid)-90:].values
inputs = inputs.reshape(-1,1)
inputs  = scaler.transform(inputs)
inputs.shape
X_test = []
for i in range(90,inputs.shape[0]):
    X_test.append(inputs[i-90:i,0])
X_test = np.array(X_test)

"""Initializing the LSTM model and 2nd, 3rd and 4th LSTM layer each with a Dropout Layer. The layers contain 50 neurons and with a Dropout rate of 20%, twenty percent of 50 neurons will be ignored randomly during each iteration

Finally, an output layer is added with 1 as an output dimension (as we are predicting the close price)


We use Stochastic Gradient Descent algorithm to compile the model and use mean squared error ad loss function

# **Making the model**
"""

model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))
model.add(Dropout(rate = 0.3))

model.add(LSTM(units=50, return_sequences = True))
model.add(Dropout(rate = 0.3))

model.add(LSTM(units=50, return_sequences = True))
model.add(Dropout(rate = 0.3))

model.add(LSTM(units=50, return_sequences = False))
model.add(Dropout(rate = 0.3))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
#we use standard adam's optimizer

"""We train the model for 100 epochs"""

model.fit(x_train, y_train, epochs=100, batch_size=128, verbose=1)

X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))
preds = model.predict(X_test)
preds = scaler.inverse_transform(preds)

preds

valid

print(valid[-1],preds[-1])

rms=np.sqrt(np.mean(np.power((valid-preds),2)))

rms
#it is the standard deviation of the residuals (prediction errors)

train = all_data[2000:4500]
valid = all_data[4500:]
valid['Predictions'] = preds
plt.figure(figsize=(20,8))
plt.plot(train['Close'])
plt.plot(valid['Close'], color = 'blue', label = 'Real Price')
plt.plot(valid['Predictions'], color = 'red', label = 'Predicted Price')
plt.title('HDFCBANK price prediction')
plt.legend()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
plt.style.use("fivethirtyeight")
# %matplotlib inline
# For reading stock data from yahoo
from pandas_datareader.data import DataReader
# For time stamps
from datetime import datetime

df = pd.read_csv(data)

df.info()

df.describe()

from keras.models import load_model
model.save('HDFCBANK.h5')  # creates a HDF5 file

!ls

from google.colab import files
files.download("HDFCBANK.h5")

